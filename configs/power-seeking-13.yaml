# Model and Data
model_name_or_path: "google/gemma-3-1b-it"
behavior: "power-seeking"
layer: [13]

# Training Hyperparameters
learning_rate: 1.0e-2
num_train_epochs: 20
beta: 0.05
per_device_train_batch_size: 4
gradient_accumulation_steps: 1
optimizer_type: "adamw_torch"
lr_scheduler_type: "cosine"
warmup_steps: 20

# Logging and Instrumentation
report_to: "wandb"
logging_steps: 1
max_length: 2048
max_prompt_length: 2048